{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r6ujQRIiS2Ct"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim import models\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dakDvRYRT49g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I personally have 5 days sets and have also bo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fabulous and worth every penny. Used for clean...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A must if you love garlic on tomato marinara s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Worth every penny! Buy one now and be a pizza ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating  rating\n",
       "0                Beautiful.  Looks great on counter.          5.0       1\n",
       "1  I personally have 5 days sets and have also bo...          5.0       1\n",
       "2  Fabulous and worth every penny. Used for clean...          5.0       1\n",
       "3  A must if you love garlic on tomato marinara s...          5.0       1\n",
       "4  Worth every penny! Buy one now and be a pizza ...          5.0       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing previously stored dataset of \n",
    "\n",
    "data = pd.read_csv('amazon_reviews_us_Kitchen_v1_00.tsv', error_bad_lines=False, warn_bad_lines=False, sep='\\t')\n",
    "df = data[['review_body', 'star_rating']]\n",
    "df['rating'] = df['star_rating'].apply(lambda x: 1 if x > 3 else (2 if x<3 else 3))\n",
    "#df['rating'] = df['star_rating'].apply(lambda x: 3 if x==3 else df['rating'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gD-UNrF-Ur_x"
   },
   "outputs": [],
   "source": [
    "# Loading inbuilt w2v model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pNl4uykIqIVr"
   },
   "outputs": [],
   "source": [
    "# Saving and loading inbuilt model from directory to avoid download everytime\n",
    "df1 = df.query(' star_rating == 1').sample(n=50000)\n",
    "df2 = df.query(' star_rating == 2').sample(n=50000)\n",
    "df3 = df.query(' star_rating == 3').sample(n=50000)\n",
    "df4 = df.query(' star_rating == 4').sample(n=50000)\n",
    "df5 = df.query(' star_rating == 5').sample(n=50000)\n",
    "frames = [df1, df2, df3, df4, df5]\n",
    "df = pd.concat(frames)\n",
    "df = df.sample(frac=1)\n",
    "wv.save(\"google_news.d2v\")\n",
    "#wv = KeyedVectors.load(\"google_news.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E4_dYhwyZLgE"
   },
   "outputs": [],
   "source": [
    "# Taking 2 sample rows\n",
    "df.head()\n",
    "wv_sample = df['review_body'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1r3fKKTFYB4I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Pot\n",
      "Word: dies\n",
      "Word: not\n",
      "Word: age\n",
      "Word: well\n",
      "Word: on\n",
      "The word a does not appear in this model\n"
     ]
    }
   ],
   "source": [
    "# Obtaining vectors for sample sentences using word2vec-google-news-300 model\n",
    "\n",
    "try:\n",
    "  for sentence in wv_sample:\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "      v = wv[word]\n",
    "      print(\"Word: \" + word)\n",
    "      #print(v)\n",
    "    \n",
    "except KeyError:\n",
    "    print(\"The word \" + word + \" does not appear in this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jvty1PgR9EvR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73005176\n",
      "0.68360925\n"
     ]
    }
   ],
   "source": [
    "wv[\"king\"]\n",
    "wv[\"man\"]\n",
    "va=wv[\"king\"]-wv[\"man\"]+wv[\"woman\"]\n",
    "vb=wv[\"queen\"]\n",
    "va1=wv[\"nice\"]\n",
    "vb1=wv[\"good\"]\n",
    "print(np.dot(va,vb)/(np.linalg.norm(va)* np.linalg.norm(vb)))\n",
    "print(np.dot(va1,vb1)/(np.linalg.norm(va1)* np.linalg.norm(vb1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kWrYKCOo9EvS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CHDbGJe_jr92"
   },
   "outputs": [],
   "source": [
    "# Splitting reviews into a list of words - tokenizing\n",
    "import re\n",
    "df[\"review_body\"] = df['review_body'].str.lower()\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "  return cleantext\n",
    "\n",
    "df['review_body'] = df['review_body'].apply(lambda x: cleanhtml(x))\n",
    "df['review_body'] = df['review_body'].str.replace('[^ a-zA-Z]', '')\n",
    "\n",
    "def contractionfunction(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "df['review_body'] = df.review_body.apply(lambda row: contractionfunction(row))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['review_body'] = df['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vw0NDlA29EvU"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output\n",
    "\n",
    "df['review_body'] = df.review_body.apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hG5rzcJt9EvU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "df_testing=df[[\"review_body\", \"rating\"]]\n",
    "df_testing[\"review_body\"] = df_testing['review_body'].str.replace('[^\\w\\s]','')\n",
    "df_testing['review_body']=df_testing['review_body'].apply(lambda x: re.sub(\"\\d+\",\"\", x))\n",
    "df_testing['review_body']=df_testing['review_body'].apply(lambda x: x.strip())\n",
    "df_testing['review_body'] = df_testing['review_body'].replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "RA6EPJaJ9EvV"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for i in df_testing['review_body']:\n",
    "  if type(i).__name__ == \"str\":\n",
    "    sentences.append(i.split())\n",
    "mymodel = models.Word2Vec(sentences = sentences, min_count = 10, size = 300, window = 11, workers = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "xAodrRbX9EvV"
   },
   "outputs": [],
   "source": [
    "mymodel.save(\"mymodel.d2v\")\n",
    "#mymodel = KeyedVectors.load(\"mymodel.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14903, 300)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cB34RNchoksL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score for idea from inbult model:  [('good', 0.7190051078796387), ('terrible', 0.6828612089157104), ('horrible', 0.6702598333358765), ('Bad', 0.669891893863678), ('lousy', 0.6647640466690063)]\n",
      "Similarity score for idea from custom model:  [('good', 0.5042324662208557), ('terrible', 0.5006387233734131), ('awful', 0.477702796459198), ('horrible', 0.44045141339302063), ('poor', 0.43576285243034363)]\n",
      "Similarity score for perfect from inbult model:  [('incredible', 0.9054000973701477), ('awesome', 0.8282865285873413), ('unbelievable', 0.8201264142990112), ('fantastic', 0.7789871096611023), ('phenomenal', 0.7642048001289368)]\n",
      "Similarity score for perfect from custom model:  [('awesome', 0.7773792147636414), ('fantastic', 0.7667995691299438), ('wonderful', 0.6966757774353027), ('fabulous', 0.6923918724060059), ('amazed', 0.6404046416282654)]\n"
     ]
    }
   ],
   "source": [
    "# Comparing similarity (top 5) for 2 sample words from dataset, for inbuilt and custom models respectively\n",
    "\n",
    "print(\"Similarity score for idea from inbult model: \", wv.most_similar('bad', topn = 5))\n",
    "print(\"Similarity score for idea from custom model: \", mymodel.wv.most_similar('bad', topn = 5))\n",
    "\n",
    "print(\"Similarity score for perfect from inbult model: \", wv.most_similar('amazing', topn = 5))\n",
    "print(\"Similarity score for perfect from custom model: \", mymodel.wv.most_similar('amazing', topn = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = df_testing[df_testing['rating'] !=3]\n",
    "df_binary.dropna()\n",
    "df_binary_2=df_binary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q38nAFZ59EvX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: Mean of empty slice.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "wv_trained = pd.DataFrame()\n",
    "mymodel_trained = pd.DataFrame()\n",
    "\n",
    "def gentrain(s,model):\n",
    "    word_list = train(s,model)\n",
    "    word_list = np.array(word_list)\n",
    "    word_list = word_list.mean(axis=0)\n",
    "    return word_list\n",
    "\n",
    "def train(s,model):\n",
    "    words=s.split()\n",
    "    each_word=[]\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            each_word.append(model[word])\n",
    "        else:\n",
    "            each_word.append(np.zeros(300))\n",
    "    return each_word\n",
    "\n",
    "    \n",
    "wv_trained['review_body']=df_binary['review_body'].apply(lambda row: gentrain(row, wv))\n",
    "mymodel_trained['review_body']=df_binary['review_body'].apply(lambda row: gentrain(row, mymodel.wv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    n_list=[]\n",
    "    for i in df:\n",
    "        i=i.tolist()\n",
    "        if type(i).__name__=='float':\n",
    "            i=[0.0 for i in range(300)]\n",
    "        n_list.append(i)\n",
    "    y=np.array([np.array(x) for x in n_list])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ggxSk8tB9EvX"
   },
   "outputs": [],
   "source": [
    "n_wv_trained=normalize(wv_trained['review_body'])\n",
    "n_mymodel_trained=normalize(mymodel_trained['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary['rating'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HH-VJ2mE9EvX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "avg_my_model_x_train, avg_my_model_x_test, avg_my_model_y_train, avg_my_model_y_test = train_test_split(n_mymodel_trained, df_binary[\"rating\"],test_size=0.2, random_state=0)\n",
    "avg_wv_x_train, avg_wv_x_test, avg_wv_y_train, avg_wv_y_test = train_test_split(n_wv_trained, df_binary[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_my_model_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "XcPh2mMD9EvZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822\n",
      "0.821\n"
     ]
    }
   ],
   "source": [
    "#CUSTOM MODEL Accuracy using Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(avg_my_model_x_train, avg_my_model_y_train)\n",
    "\n",
    "perceptron_pred_train = clf.predict(avg_my_model_x_train)\n",
    "perceptron_pred_test = clf.predict(avg_my_model_x_test)\n",
    "\n",
    "print('%.3f' % accuracy_score(avg_my_model_y_train, perceptron_pred_train))\n",
    "print('%.3f' % accuracy_score(avg_my_model_y_test, perceptron_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714\n",
      "0.716\n"
     ]
    }
   ],
   "source": [
    "#WV MODEL Accuracy using Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "clf1 = Perceptron(tol=1e-3, random_state=0)\n",
    "clf1.fit(avg_wv_x_train, avg_wv_y_train)\n",
    "\n",
    "perceptron_pred_train = clf1.predict(avg_wv_x_train)\n",
    "perceptron_pred_test = clf1.predict(avg_wv_x_test)\n",
    "\n",
    "print('%.3f' % accuracy_score(avg_wv_y_train, perceptron_pred_train))\n",
    "print('%.3f' % accuracy_score(avg_wv_y_test, perceptron_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF_IDF splitting\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_testing['review_body'])\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, df_testing['rating'],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n",
      "0.647\n"
     ]
    }
   ],
   "source": [
    "#TF_IDF accuracy using Perceptron\n",
    "clf2 = Perceptron(tol=1e-3, random_state=0)\n",
    "clf2.fit(train_X, train_Y)\n",
    "\n",
    "perceptron_pred_train = clf2.predict(train_X)\n",
    "perceptron_pred_test = clf2.predict(test_X)\n",
    "\n",
    "print('%.3f' % accuracy_score(train_Y,perceptron_pred_train))\n",
    "print('%.3f' % accuracy_score(test_Y,perceptron_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.861\n",
      "0.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Custom model accuracy using SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(avg_my_model_x_train, avg_my_model_y_train)\n",
    "\n",
    "SVM_pred_train = clf.predict(avg_my_model_x_train)\n",
    "SVM_pred_test = clf.predict(avg_my_model_x_test)\n",
    "\n",
    "print('%.3f' % accuracy_score(avg_my_model_y_train, SVM_pred_train))\n",
    "print('%.3f' % accuracy_score(avg_my_model_y_test, SVM_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822\n",
      "0.820\n"
     ]
    }
   ],
   "source": [
    "#WV model accuracy using SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(avg_wv_x_train, avg_wv_y_train)\n",
    "SVM_pred_train = clf.predict(avg_wv_x_train)\n",
    "SVM_pred_test = clf.predict(avg_wv_x_test)\n",
    "\n",
    "print('%.3f' % accuracy_score(avg_wv_y_train, SVM_pred_train))\n",
    "print('%.3f' % accuracy_score(avg_wv_y_test, SVM_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801\n",
      "0.707\n"
     ]
    }
   ],
   "source": [
    "#TF_IDF model accuracy using SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(train_X, train_Y)\n",
    "SVM_pred_train = clf.predict(train_X)\n",
    "SVM_pred_test = clf.predict(test_X)\n",
    "\n",
    "print('%.3f' % accuracy_score(train_Y, SVM_pred_train))\n",
    "print('%.3f' % accuracy_score(test_Y, SVM_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "ptdf=df[df['star_rating']!=\"3\"]\n",
    "msk = np.random.rand(len(ptdf)) < 0.8\n",
    "pt_train = ptdf[msk]\n",
    "pt_test = ptdf[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>509400</th>\n",
       "      <td>pot dy age well gas stove surface corrosion st...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245218</th>\n",
       "      <td>broke first time used snapped half middle piec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676882</th>\n",
       "      <td>recieved item plastic handle broken handle mad...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845252</th>\n",
       "      <td>got coffee maker spent brewed first cup coffee...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865398</th>\n",
       "      <td>love popcorn popper year later still making ba...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  star_rating  \\\n",
       "509400   pot dy age well gas stove surface corrosion st...          2.0   \n",
       "1245218  broke first time used snapped half middle piec...          1.0   \n",
       "2676882  recieved item plastic handle broken handle mad...          1.0   \n",
       "4845252  got coffee maker spent brewed first cup coffee...          2.0   \n",
       "3865398  love popcorn popper year later still making ba...          5.0   \n",
       "\n",
       "         rating  \n",
       "509400        2  \n",
       "1245218       2  \n",
       "2676882       2  \n",
       "4845252       2  \n",
       "3865398       1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_binary[\"rating\"]=df_binary[\"rating\"].apply(lambda x: 0 if x==1 else x).any()\n",
    "df_binary[\"rating\"]=df_binary[\"rating\"].apply(lambda x: 1 if x==2 else x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_x_train, avg_wv_x_test, avg_wv_y_train, avg_wv_y_test = train_test_split(n_wv_trained, df_binary[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(avg_wv_x_train)\n",
    "y_train = torch.tensor(avg_wv_y_train.astype(np.float32).values, dtype=torch.long)\n",
    "x_test = torch.tensor(avg_wv_x_test)\n",
    "y_test = torch.tensor(avg_wv_y_test.astype(np.float32).values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(x_train)):\n",
    "    train_data.append([x_train[i], y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in range(len(x_test)):\n",
    "    test_data.append([x_test[i], y_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "#indices = list(range(num_train))\n",
    "indices = [i for i in range(num_train)]\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        \n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        #x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        #x = self.softmax(x)\n",
    "        return(F.log_softmax(x,dim=1))\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 40\n",
    "\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for data, target in train_loader:\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data.float())\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data.float())\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # update running validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch+1, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, \n",
    "    num_workers=num_workers)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    total = len(test_data)\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in test_loader:\n",
    "            reviews, labels = data\n",
    "            outputs = model(reviews)\n",
    "            _,predicted = torch.max(outputs.data,1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100 %\n"
     ]
    }
   ],
   "source": [
    "#WV Model accuracy using Feedforward Neural network for binary classification\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_my_model_x_train, avg_my_model_x_test, avg_my_model_y_train, avg_my_model_y_test = train_test_split(n_mymodel_trained, df_binary[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(avg_my_model_x_train)\n",
    "y_train = torch.tensor(avg_my_model_y_train.astype(np.float32).values, dtype=torch.long)\n",
    "x_test = torch.tensor(avg_my_model_x_test)\n",
    "y_test = torch.tensor(avg_my_model_y_test.astype(np.float32).values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(x_train)):\n",
    "    train_data.append([x_train[i], y_train[i]])\n",
    "test_data = []\n",
    "for i in range(len(x_test)):\n",
    "    test_data.append([x_test[i], y_test[i]])\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "#indices = list(range(num_train))\n",
    "indices = [i for i in range(num_train)]\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000130 \tValidation Loss: 0.000031\n",
      "Validation loss decreased (inf --> 0.000031).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.000126 \tValidation Loss: 0.000030\n",
      "Validation loss decreased (0.000031 --> 0.000030).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.000122 \tValidation Loss: 0.000029\n",
      "Validation loss decreased (0.000030 --> 0.000029).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.000118 \tValidation Loss: 0.000028\n",
      "Validation loss decreased (0.000029 --> 0.000028).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.000115 \tValidation Loss: 0.000027\n",
      "Validation loss decreased (0.000028 --> 0.000027).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.000111 \tValidation Loss: 0.000027\n",
      "Validation loss decreased (0.000027 --> 0.000027).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.000108 \tValidation Loss: 0.000026\n",
      "Validation loss decreased (0.000027 --> 0.000026).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.000105 \tValidation Loss: 0.000025\n",
      "Validation loss decreased (0.000026 --> 0.000025).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.000102 \tValidation Loss: 0.000024\n",
      "Validation loss decreased (0.000025 --> 0.000024).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.000099 \tValidation Loss: 0.000023\n",
      "Validation loss decreased (0.000024 --> 0.000023).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.000096 \tValidation Loss: 0.000023\n",
      "Validation loss decreased (0.000023 --> 0.000023).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.000094 \tValidation Loss: 0.000022\n",
      "Validation loss decreased (0.000023 --> 0.000022).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.000091 \tValidation Loss: 0.000022\n",
      "Validation loss decreased (0.000022 --> 0.000022).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.000089 \tValidation Loss: 0.000021\n",
      "Validation loss decreased (0.000022 --> 0.000021).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.000087 \tValidation Loss: 0.000020\n",
      "Validation loss decreased (0.000021 --> 0.000020).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.000085 \tValidation Loss: 0.000020\n",
      "Validation loss decreased (0.000020 --> 0.000020).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.000083 \tValidation Loss: 0.000019\n",
      "Validation loss decreased (0.000020 --> 0.000019).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.000081 \tValidation Loss: 0.000019\n",
      "Validation loss decreased (0.000019 --> 0.000019).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.000079 \tValidation Loss: 0.000018\n",
      "Validation loss decreased (0.000019 --> 0.000018).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.000077 \tValidation Loss: 0.000018\n",
      "Validation loss decreased (0.000018 --> 0.000018).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.000076 \tValidation Loss: 0.000018\n",
      "Validation loss decreased (0.000018 --> 0.000018).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.000074 \tValidation Loss: 0.000017\n",
      "Validation loss decreased (0.000018 --> 0.000017).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.000072 \tValidation Loss: 0.000017\n",
      "Validation loss decreased (0.000017 --> 0.000017).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.000071 \tValidation Loss: 0.000016\n",
      "Validation loss decreased (0.000017 --> 0.000016).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.000069 \tValidation Loss: 0.000016\n",
      "Validation loss decreased (0.000016 --> 0.000016).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.000067 \tValidation Loss: 0.000015\n",
      "Validation loss decreased (0.000016 --> 0.000015).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.000066 \tValidation Loss: 0.000015\n",
      "Validation loss decreased (0.000015 --> 0.000015).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.000065 \tValidation Loss: 0.000015\n",
      "Validation loss decreased (0.000015 --> 0.000015).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.000063 \tValidation Loss: 0.000014\n",
      "Validation loss decreased (0.000015 --> 0.000014).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.000062 \tValidation Loss: 0.000014\n",
      "Validation loss decreased (0.000014 --> 0.000014).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.000061 \tValidation Loss: 0.000014\n",
      "Validation loss decreased (0.000014 --> 0.000014).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.000060 \tValidation Loss: 0.000013\n",
      "Validation loss decreased (0.000014 --> 0.000013).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.000058 \tValidation Loss: 0.000013\n",
      "Validation loss decreased (0.000013 --> 0.000013).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.000057 \tValidation Loss: 0.000013\n",
      "Validation loss decreased (0.000013 --> 0.000013).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.000056 \tValidation Loss: 0.000013\n",
      "Validation loss decreased (0.000013 --> 0.000013).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.000055 \tValidation Loss: 0.000012\n",
      "Validation loss decreased (0.000013 --> 0.000012).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.000054 \tValidation Loss: 0.000012\n",
      "Validation loss decreased (0.000012 --> 0.000012).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.000054 \tValidation Loss: 0.000012\n",
      "Validation loss decreased (0.000012 --> 0.000012).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.000053 \tValidation Loss: 0.000012\n",
      "Validation loss decreased (0.000012 --> 0.000012).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.000052 \tValidation Loss: 0.000011\n",
      "Validation loss decreased (0.000012 --> 0.000011).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100 %\n"
     ]
    }
   ],
   "source": [
    "#Custom model accuracy using Feed forward neural network for binary classification\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ternary=df_testing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>509400</th>\n",
       "      <td>pot dy age well gas stove surface corrosion st...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245218</th>\n",
       "      <td>broke first time used snapped half middle piec...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676882</th>\n",
       "      <td>recieved item plastic handle broken handle mad...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845252</th>\n",
       "      <td>got coffee maker spent brewed first cup coffee...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865398</th>\n",
       "      <td>love popcorn popper year later still making ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398974</th>\n",
       "      <td>utensil came handy made well sturdy practical ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204428</th>\n",
       "      <td>shipped fast exactly looking plastic hurricane...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084990</th>\n",
       "      <td>keep water cold decent size fit cup holder loo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696096</th>\n",
       "      <td>ordered x pack needing total card shipped tota...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049554</th>\n",
       "      <td>doubt great product really work well keep spla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body  rating\n",
       "509400   pot dy age well gas stove surface corrosion st...       2\n",
       "1245218  broke first time used snapped half middle piec...       2\n",
       "2676882  recieved item plastic handle broken handle mad...       2\n",
       "4845252  got coffee maker spent brewed first cup coffee...       2\n",
       "3865398  love popcorn popper year later still making ba...       1\n",
       "...                                                    ...     ...\n",
       "3398974  utensil came handy made well sturdy practical ...       1\n",
       "4204428  shipped fast exactly looking plastic hurricane...       1\n",
       "1084990  keep water cold decent size fit cup holder loo...       1\n",
       "2696096  ordered x pack needing total card shipped tota...       2\n",
       "4049554  doubt great product really work well keep spla...       1\n",
       "\n",
       "[250000 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ternary.dropna()\n",
    "df_ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidmitta/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: Mean of empty slice.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "wv1 = pd.DataFrame()\n",
    "mymodel1 = pd.DataFrame()\n",
    "\n",
    "def gentrain(s,model):\n",
    "    word_list = train(s,model)\n",
    "    word_list = np.array(word_list)\n",
    "    word_list = word_list.mean(axis=0)\n",
    "    return word_list\n",
    "\n",
    "def train(s,model):\n",
    "    words=s.split()\n",
    "    each_word=[]\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            each_word.append(model[word])\n",
    "        else:\n",
    "            each_word.append(np.zeros(300))\n",
    "    return each_word\n",
    "\n",
    "wv1['review_body']=df_ternary['review_body'].apply(lambda row: gentrain(row, wv))\n",
    "mymodel1['review_body']=df_ternary['review_body'].apply(lambda row: gentrain(row, mymodel.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wv_trained=normalize(wv1['review_body'])\n",
    "n_mymodel_trained=normalize(mymodel1['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 300)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_wv_trained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ternary['rating']=df_ternary['rating'].replace(1,0)\n",
    "df_ternary['rating']=df_ternary['rating'].replace(2,1)\n",
    "df_ternary['rating']=df_ternary['rating'].replace(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_body, rating]\n",
       "Index: []"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ternary.query(\"rating==3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_my_model_x_train, avg_my_model_x_test, avg_my_model_y_train, avg_my_model_y_test = train_test_split(n_mymodel_trained, df_ternary[\"rating\"],test_size=0.2, random_state=0)\n",
    "avg_wv_x_train, avg_wv_x_test, avg_wv_y_train, avg_wv_y_test = train_test_split(n_wv_trained, df_ternary[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(avg_my_model_x_train)\n",
    "y_train = torch.tensor(avg_my_model_y_train.astype(np.float32).values, dtype=torch.long)\n",
    "x_test = torch.tensor(avg_my_model_x_test)\n",
    "y_test = torch.tensor(avg_my_model_y_test.astype(np.float32).values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200000, 300])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = []\n",
    "for i in range(len(x_train)):\n",
    "    train_data.append([x_train[i], y_train[i]])\n",
    "test_data = []\n",
    "for i in range(len(x_test)):\n",
    "    test_data.append([x_test[i], y_test[i]])\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "#indices = list(range(num_train))\n",
    "indices = [i for i in range(num_train)]\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TernaryNet, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        \n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        #x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        #x = self.softmax(x)\n",
    "        return(x)\n",
    "\n",
    "# initialize the NN\n",
    "model = TernaryNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.896904 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (inf --> 0.224210).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.896886 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.896876 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.896881 \tValidation Loss: 0.224210\n",
      "Epoch: 5 \tTraining Loss: 0.896860 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.896884 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.896873 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.896862 \tValidation Loss: 0.224210\n",
      "Epoch: 9 \tTraining Loss: 0.896868 \tValidation Loss: 0.224210\n",
      "Epoch: 10 \tTraining Loss: 0.896859 \tValidation Loss: 0.224210\n",
      "Epoch: 11 \tTraining Loss: 0.896916 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.896919 \tValidation Loss: 0.224210\n",
      "Epoch: 13 \tTraining Loss: 0.896857 \tValidation Loss: 0.224210\n",
      "Epoch: 14 \tTraining Loss: 0.896803 \tValidation Loss: 0.224210\n",
      "Epoch: 15 \tTraining Loss: 0.896880 \tValidation Loss: 0.224210\n",
      "Epoch: 16 \tTraining Loss: 0.896846 \tValidation Loss: 0.224210\n",
      "Epoch: 17 \tTraining Loss: 0.896864 \tValidation Loss: 0.224210\n",
      "Epoch: 18 \tTraining Loss: 0.896840 \tValidation Loss: 0.224210\n",
      "Epoch: 19 \tTraining Loss: 0.896892 \tValidation Loss: 0.224210\n",
      "Epoch: 20 \tTraining Loss: 0.896948 \tValidation Loss: 0.224210\n",
      "Epoch: 21 \tTraining Loss: 0.896842 \tValidation Loss: 0.224210\n",
      "Epoch: 22 \tTraining Loss: 0.896897 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.896840 \tValidation Loss: 0.224210\n",
      "Epoch: 24 \tTraining Loss: 0.896864 \tValidation Loss: 0.224210\n",
      "Epoch: 25 \tTraining Loss: 0.896888 \tValidation Loss: 0.224210\n",
      "Epoch: 26 \tTraining Loss: 0.896922 \tValidation Loss: 0.224210\n",
      "Epoch: 27 \tTraining Loss: 0.896815 \tValidation Loss: 0.224210\n",
      "Epoch: 28 \tTraining Loss: 0.896855 \tValidation Loss: 0.224210\n",
      "Epoch: 29 \tTraining Loss: 0.896873 \tValidation Loss: 0.224210\n",
      "Epoch: 30 \tTraining Loss: 0.896862 \tValidation Loss: 0.224210\n",
      "Epoch: 31 \tTraining Loss: 0.896818 \tValidation Loss: 0.224210\n",
      "Epoch: 32 \tTraining Loss: 0.896867 \tValidation Loss: 0.224210\n",
      "Epoch: 33 \tTraining Loss: 0.896835 \tValidation Loss: 0.224210\n",
      "Epoch: 34 \tTraining Loss: 0.896846 \tValidation Loss: 0.224210\n",
      "Epoch: 35 \tTraining Loss: 0.896766 \tValidation Loss: 0.224210\n",
      "Validation loss decreased (0.224210 --> 0.224210).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.896801 \tValidation Loss: 0.224210\n",
      "Epoch: 37 \tTraining Loss: 0.896844 \tValidation Loss: 0.224210\n",
      "Epoch: 38 \tTraining Loss: 0.896860 \tValidation Loss: 0.224210\n",
      "Epoch: 39 \tTraining Loss: 0.896845 \tValidation Loss: 0.224210\n",
      "Epoch: 40 \tTraining Loss: 0.896867 \tValidation Loss: 0.224210\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "#Custom model accuracy using Feed forward Neural network for ternary classification\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(avg_wv_x_train)\n",
    "y_train = torch.tensor(avg_wv_y_train.astype(np.float32).values, dtype=torch.long)\n",
    "x_test = torch.tensor(avg_wv_x_test)\n",
    "y_test = torch.tensor(avg_wv_y_test.astype(np.float32).values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(x_train)):\n",
    "    train_data.append([x_train[i], y_train[i]])\n",
    "test_data = []\n",
    "for i in range(len(x_test)):\n",
    "    test_data.append([x_test[i], y_test[i]])\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "#indices = list(range(num_train))\n",
    "indices = [i for i in range(num_train)]\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.906087 \tValidation Loss: 0.226728\n",
      "Validation loss decreased (inf --> 0.226728).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.906026 \tValidation Loss: 0.226728\n",
      "Validation loss decreased (0.226728 --> 0.226728).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.906079 \tValidation Loss: 0.226728\n",
      "Validation loss decreased (0.226728 --> 0.226728).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.906048 \tValidation Loss: 0.226728\n",
      "Epoch: 5 \tTraining Loss: 0.906087 \tValidation Loss: 0.226728\n",
      "Epoch: 6 \tTraining Loss: 0.906043 \tValidation Loss: 0.226728\n",
      "Epoch: 7 \tTraining Loss: 0.906053 \tValidation Loss: 0.226728\n",
      "Epoch: 8 \tTraining Loss: 0.906046 \tValidation Loss: 0.226728\n",
      "Epoch: 9 \tTraining Loss: 0.906056 \tValidation Loss: 0.226728\n",
      "Validation loss decreased (0.226728 --> 0.226728).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.906067 \tValidation Loss: 0.226728\n",
      "Epoch: 11 \tTraining Loss: 0.906074 \tValidation Loss: 0.226728\n",
      "Epoch: 12 \tTraining Loss: 0.906060 \tValidation Loss: 0.226728\n",
      "Epoch: 13 \tTraining Loss: 0.906037 \tValidation Loss: 0.226728\n",
      "Epoch: 14 \tTraining Loss: 0.906044 \tValidation Loss: 0.226728\n",
      "Epoch: 15 \tTraining Loss: 0.906062 \tValidation Loss: 0.226728\n",
      "Epoch: 16 \tTraining Loss: 0.906046 \tValidation Loss: 0.226728\n",
      "Epoch: 17 \tTraining Loss: 0.906072 \tValidation Loss: 0.226728\n",
      "Epoch: 18 \tTraining Loss: 0.906051 \tValidation Loss: 0.226728\n",
      "Epoch: 19 \tTraining Loss: 0.906055 \tValidation Loss: 0.226728\n",
      "Epoch: 20 \tTraining Loss: 0.906082 \tValidation Loss: 0.226728\n",
      "Epoch: 21 \tTraining Loss: 0.906041 \tValidation Loss: 0.226728\n",
      "Epoch: 22 \tTraining Loss: 0.906043 \tValidation Loss: 0.226728\n",
      "Epoch: 23 \tTraining Loss: 0.906089 \tValidation Loss: 0.226728\n",
      "Epoch: 24 \tTraining Loss: 0.906063 \tValidation Loss: 0.226728\n",
      "Epoch: 25 \tTraining Loss: 0.906059 \tValidation Loss: 0.226728\n",
      "Epoch: 26 \tTraining Loss: 0.906068 \tValidation Loss: 0.226728\n",
      "Epoch: 27 \tTraining Loss: 0.906067 \tValidation Loss: 0.226728\n",
      "Epoch: 28 \tTraining Loss: 0.906065 \tValidation Loss: 0.226728\n",
      "Epoch: 29 \tTraining Loss: 0.906053 \tValidation Loss: 0.226728\n",
      "Epoch: 30 \tTraining Loss: 0.906040 \tValidation Loss: 0.226728\n",
      "Epoch: 31 \tTraining Loss: 0.906055 \tValidation Loss: 0.226728\n",
      "Epoch: 32 \tTraining Loss: 0.906067 \tValidation Loss: 0.226728\n",
      "Epoch: 33 \tTraining Loss: 0.906064 \tValidation Loss: 0.226728\n",
      "Epoch: 34 \tTraining Loss: 0.906053 \tValidation Loss: 0.226728\n",
      "Epoch: 35 \tTraining Loss: 0.906067 \tValidation Loss: 0.226728\n",
      "Epoch: 36 \tTraining Loss: 0.906048 \tValidation Loss: 0.226728\n",
      "Epoch: 37 \tTraining Loss: 0.906071 \tValidation Loss: 0.226728\n",
      "Epoch: 38 \tTraining Loss: 0.906085 \tValidation Loss: 0.226728\n",
      "Epoch: 39 \tTraining Loss: 0.906072 \tValidation Loss: 0.226728\n",
      "Epoch: 40 \tTraining Loss: 0.906037 \tValidation Loss: 0.226728\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "#WV model accuracy using feedforward Neural network for ternary classification\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genpretrain_ten(s,wv):\n",
    "    list = pretrain(s,wv)\n",
    "    #list = np.array(list)\n",
    "    length = len(list)\n",
    "    if length<10:\n",
    "        diff = 10 - length\n",
    "        newlist = []\n",
    "        for i in list:\n",
    "            \n",
    "            newlist = np.concatenate((newlist, i), axis=None)\n",
    "        for i in range(diff):\n",
    "            \n",
    "            newlist = np.concatenate((newlist,[0.0 for i in range(300)]), axis = None)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        newlist=[]\n",
    "        for i in list[:10]:\n",
    "            newlist = np.concatenate((newlist, i), axis=None)\n",
    "    return(newlist)\n",
    "\n",
    "\n",
    "def normalize_ten(df):\n",
    "    list= []\n",
    "    for i in df:\n",
    "        list.append(np.array(i))\n",
    "    list = np.array(list)\n",
    "    return list\n",
    "\n",
    "def gentrain(s,model):\n",
    "    word_list = train(s,model)\n",
    "    word_list = np.array(word_list)\n",
    "    word_list = word_list.mean(axis=0)\n",
    "    return word_list\n",
    "\n",
    "def train(s,model):\n",
    "    words=s.split()\n",
    "    each_word=[]\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            each_word.append(model[word])\n",
    "        else:\n",
    "            each_word.append(np.zeros(300))\n",
    "    return each_word\n",
    "\n",
    "\n",
    "def pretrain(s,wv):\n",
    "    j = s.split()\n",
    "    list= []\n",
    "    for word in j :\n",
    "        if word not in wv:\n",
    "            list.append([0 for i in range(300)])\n",
    "        else:\n",
    "            list.append(wv[word])\n",
    "    \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_10 = pd.DataFrame()\n",
    "mymodel_10 = pd.DataFrame()\n",
    "\n",
    "wv_10['review_body']=df_ternary['review_body'].apply(lambda row: genpretrain_ten(row, wv))\n",
    "mymodel_10['review_body']=df_ternary['review_body'].apply(lambda row: genpretrain_ten(row, mymodel.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wv_trained=normalize_ten(wv_10['review_body'])\n",
    "n_mymodel_trained=normalize_ten(mymodel_10['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_my_model_x_train, avg_my_model_x_test, avg_my_model_y_train, avg_my_model_y_test = train_test_split(n_mymodel_trained, df_ternary[\"rating\"],test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_x_train, avg_wv_x_test, avg_wv_y_train, avg_wv_y_test = train_test_split(n_wv_trained, df_ternary[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(avg_wv_x_train)\n",
    "y_train = torch.tensor(avg_wv_y_train.astype(np.float32).values, dtype=torch.long)\n",
    "x_test = torch.tensor(avg_wv_x_test)\n",
    "y_test = torch.tensor(avg_wv_y_test.astype(np.float32).values, dtype=torch.long)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(x_train)):\n",
    "    train_data.append([x_train[i], y_train[i]])\n",
    "test_data = []\n",
    "for i in range(len(x_test)):\n",
    "    test_data.append([x_test[i], y_test[i]])\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "#indices = list(range(num_train))\n",
    "indices = [i for i in range(num_train)]\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryNet_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TernaryNet_10, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        \n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        #x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        #x = self.softmax(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TernaryNet_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.881601 \tValidation Loss: 0.220370\n",
      "Validation loss decreased (inf --> 0.220370).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.881581 \tValidation Loss: 0.220370\n",
      "Validation loss decreased (0.220370 --> 0.220370).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.881476 \tValidation Loss: 0.220370\n",
      "Epoch: 4 \tTraining Loss: 0.881517 \tValidation Loss: 0.220370\n",
      "Validation loss decreased (0.220370 --> 0.220370).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.881577 \tValidation Loss: 0.220370\n",
      "Validation loss decreased (0.220370 --> 0.220370).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.881553 \tValidation Loss: 0.220370\n",
      "Epoch: 7 \tTraining Loss: 0.881568 \tValidation Loss: 0.220370\n",
      "Epoch: 8 \tTraining Loss: 0.881542 \tValidation Loss: 0.220370\n",
      "Validation loss decreased (0.220370 --> 0.220370).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.881555 \tValidation Loss: 0.220370\n",
      "Epoch: 10 \tTraining Loss: 0.881468 \tValidation Loss: 0.220370\n",
      "Epoch: 11 \tTraining Loss: 0.881565 \tValidation Loss: 0.220370\n",
      "Epoch: 12 \tTraining Loss: 0.881573 \tValidation Loss: 0.220370\n",
      "Epoch: 13 \tTraining Loss: 0.881535 \tValidation Loss: 0.220370\n",
      "Epoch: 14 \tTraining Loss: 0.881536 \tValidation Loss: 0.220370\n",
      "Epoch: 15 \tTraining Loss: 0.881548 \tValidation Loss: 0.220370\n",
      "Epoch: 16 \tTraining Loss: 0.881469 \tValidation Loss: 0.220370\n",
      "Epoch: 17 \tTraining Loss: 0.881482 \tValidation Loss: 0.220370\n",
      "Epoch: 18 \tTraining Loss: 0.881605 \tValidation Loss: 0.220370\n",
      "Validation loss decreased (0.220370 --> 0.220370).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.881521 \tValidation Loss: 0.220370\n",
      "Epoch: 20 \tTraining Loss: 0.881605 \tValidation Loss: 0.220370\n",
      "Epoch: 21 \tTraining Loss: 0.881593 \tValidation Loss: 0.220370\n",
      "Epoch: 22 \tTraining Loss: 0.881476 \tValidation Loss: 0.220370\n",
      "Epoch: 23 \tTraining Loss: 0.881568 \tValidation Loss: 0.220370\n",
      "Epoch: 24 \tTraining Loss: 0.881540 \tValidation Loss: 0.220370\n",
      "Validation loss decreased (0.220370 --> 0.220370).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.881622 \tValidation Loss: 0.220370\n",
      "Epoch: 26 \tTraining Loss: 0.881578 \tValidation Loss: 0.220370\n",
      "Epoch: 27 \tTraining Loss: 0.881521 \tValidation Loss: 0.220370\n",
      "Epoch: 28 \tTraining Loss: 0.881505 \tValidation Loss: 0.220370\n",
      "Epoch: 29 \tTraining Loss: 0.881570 \tValidation Loss: 0.220370\n",
      "Epoch: 30 \tTraining Loss: 0.881553 \tValidation Loss: 0.220370\n",
      "Epoch: 31 \tTraining Loss: 0.881591 \tValidation Loss: 0.220370\n",
      "Epoch: 32 \tTraining Loss: 0.881455 \tValidation Loss: 0.220370\n",
      "Epoch: 33 \tTraining Loss: 0.881542 \tValidation Loss: 0.220370\n",
      "Epoch: 34 \tTraining Loss: 0.881534 \tValidation Loss: 0.220370\n",
      "Epoch: 35 \tTraining Loss: 0.881601 \tValidation Loss: 0.220370\n",
      "Epoch: 36 \tTraining Loss: 0.881484 \tValidation Loss: 0.220370\n",
      "Epoch: 37 \tTraining Loss: 0.881548 \tValidation Loss: 0.220370\n",
      "Epoch: 38 \tTraining Loss: 0.881498 \tValidation Loss: 0.220370\n",
      "Epoch: 39 \tTraining Loss: 0.881480 \tValidation Loss: 0.220370\n",
      "Epoch: 40 \tTraining Loss: 0.881573 \tValidation Loss: 0.220370\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "#WV model ternary classification accuracy for concatenated vectors using Feedforward neural network\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTrainandTest_ten(train_x,train_y,test_x,test_y,wv):\n",
    "\n",
    "    x_train = torch.tensor(train_x)\n",
    "    y_train = torch.tensor(train_y.astype(np.float32).values, dtype=torch.long)\n",
    "    x_test = torch.tensor(test_x)\n",
    "    y_test = torch.tensor(test_y.astype(np.float32).values, dtype=torch.long)\n",
    "    train_data = []\n",
    "    for i in range(len(x_train)):\n",
    "        train_data.append([x_train[i], y_train[i]])\n",
    "    test_data = []\n",
    "    for i in range(len(x_test)):\n",
    "        test_data.append([x_test[i], y_test[i]])\n",
    "    # number of subprocesses to use for data loading\n",
    "    num_workers = 0\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 20\n",
    "    # percentage of training set to use as validation\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    #indices = list(range(num_train))\n",
    "    indices = [i for i in range(num_train)]\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers,)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "        sampler=valid_sampler, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "        num_workers=num_workers)\n",
    "    \n",
    "    \n",
    "    return (train_data, test_data, train_loader, valid_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_loader, valid_loader, test_loader = prepareTrainandTest_ten(avg_my_model_x_train,avg_my_model_y_train,avg_my_model_x_test,avg_my_model_y_test,mymodel.wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.906275 \tValidation Loss: 0.225647\n",
      "Validation loss decreased (inf --> 0.225647).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.906103 \tValidation Loss: 0.225647\n",
      "Epoch: 3 \tTraining Loss: 0.906253 \tValidation Loss: 0.225647\n",
      "Validation loss decreased (0.225647 --> 0.225647).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.906247 \tValidation Loss: 0.225647\n",
      "Epoch: 5 \tTraining Loss: 0.906285 \tValidation Loss: 0.225647\n",
      "Epoch: 6 \tTraining Loss: 0.906019 \tValidation Loss: 0.225647\n",
      "Epoch: 7 \tTraining Loss: 0.906270 \tValidation Loss: 0.225647\n",
      "Validation loss decreased (0.225647 --> 0.225647).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.906395 \tValidation Loss: 0.225647\n",
      "Epoch: 9 \tTraining Loss: 0.906126 \tValidation Loss: 0.225647\n",
      "Epoch: 10 \tTraining Loss: 0.906000 \tValidation Loss: 0.225647\n",
      "Epoch: 11 \tTraining Loss: 0.906247 \tValidation Loss: 0.225647\n",
      "Epoch: 12 \tTraining Loss: 0.906024 \tValidation Loss: 0.225647\n",
      "Epoch: 13 \tTraining Loss: 0.906137 \tValidation Loss: 0.225647\n",
      "Epoch: 14 \tTraining Loss: 0.906134 \tValidation Loss: 0.225647\n",
      "Epoch: 15 \tTraining Loss: 0.905873 \tValidation Loss: 0.225647\n",
      "Epoch: 16 \tTraining Loss: 0.905860 \tValidation Loss: 0.225647\n",
      "Epoch: 17 \tTraining Loss: 0.906110 \tValidation Loss: 0.225647\n",
      "Epoch: 18 \tTraining Loss: 0.906045 \tValidation Loss: 0.225647\n",
      "Epoch: 19 \tTraining Loss: 0.906234 \tValidation Loss: 0.225647\n",
      "Epoch: 20 \tTraining Loss: 0.905682 \tValidation Loss: 0.225647\n",
      "Epoch: 21 \tTraining Loss: 0.906421 \tValidation Loss: 0.225647\n",
      "Epoch: 22 \tTraining Loss: 0.906243 \tValidation Loss: 0.225647\n",
      "Epoch: 23 \tTraining Loss: 0.906064 \tValidation Loss: 0.225647\n",
      "Epoch: 24 \tTraining Loss: 0.906121 \tValidation Loss: 0.225647\n",
      "Epoch: 25 \tTraining Loss: 0.906299 \tValidation Loss: 0.225647\n",
      "Epoch: 26 \tTraining Loss: 0.906317 \tValidation Loss: 0.225647\n",
      "Epoch: 27 \tTraining Loss: 0.906050 \tValidation Loss: 0.225647\n",
      "Epoch: 28 \tTraining Loss: 0.906113 \tValidation Loss: 0.225647\n",
      "Epoch: 29 \tTraining Loss: 0.906491 \tValidation Loss: 0.225647\n",
      "Epoch: 30 \tTraining Loss: 0.906384 \tValidation Loss: 0.225647\n",
      "Epoch: 31 \tTraining Loss: 0.905914 \tValidation Loss: 0.225647\n",
      "Epoch: 32 \tTraining Loss: 0.906072 \tValidation Loss: 0.225647\n",
      "Epoch: 33 \tTraining Loss: 0.906289 \tValidation Loss: 0.225647\n",
      "Epoch: 34 \tTraining Loss: 0.906014 \tValidation Loss: 0.225647\n",
      "Epoch: 35 \tTraining Loss: 0.906430 \tValidation Loss: 0.225647\n",
      "Epoch: 36 \tTraining Loss: 0.906243 \tValidation Loss: 0.225647\n",
      "Epoch: 37 \tTraining Loss: 0.906032 \tValidation Loss: 0.225647\n",
      "Validation loss decreased (0.225647 --> 0.225647).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.906127 \tValidation Loss: 0.225647\n",
      "Epoch: 39 \tTraining Loss: 0.906332 \tValidation Loss: 0.225647\n",
      "Epoch: 40 \tTraining Loss: 0.906193 \tValidation Loss: 0.225647\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 27 %\n"
     ]
    }
   ],
   "source": [
    "#Custom model ternary classification accuracy for concatenated vectors using Feedforward neural network\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary_10 = pd.DataFrame()\n",
    "df_binary_10 = df_binary_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary_10[\"rating\"]=df_binary_2[\"rating\"].apply(lambda x: 0 if x == 1 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary_10[\"rating\"]=df_binary_10[\"rating\"].apply(lambda x: 1 if x == 2 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_10_binary = pd.DataFrame()\n",
    "mymodel_10_binary = pd.DataFrame()\n",
    "\n",
    "wv_10_binary['review_body']=df_binary_10['review_body'].apply(lambda row: genpretrain_ten(row, wv))\n",
    "mymodel_10_binary['review_body']=df_binary_10['review_body'].apply(lambda row: genpretrain_ten(row, mymodel.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wv_trained=normalize_ten(wv_10_binary['review_body'])\n",
    "n_mymodel_trained=normalize_ten(mymodel_10_binary['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "avg_my_model_x_train, avg_my_model_x_test, avg_my_model_y_train, avg_my_model_y_test = train_test_split(n_mymodel_trained, df_binary_10[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_x_train, avg_wv_x_test, avg_wv_y_train, avg_wv_y_test = train_test_split(n_wv_trained, df_binary_10[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_loader, valid_loader, test_loader = prepareTrainandTest_ten(avg_wv_x_train,avg_wv_y_train,avg_wv_x_test,avg_wv_y_test,wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        \n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        #x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x.float()))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        #x = self.softmax(x)\n",
    "        return(F.log_softmax(x,dim=1))\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.555174 \tValidation Loss: 0.138792\n",
      "Validation loss decreased (inf --> 0.138792).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.555142 \tValidation Loss: 0.138792\n",
      "Validation loss decreased (0.138792 --> 0.138792).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.555162 \tValidation Loss: 0.138792\n",
      "Epoch: 4 \tTraining Loss: 0.555144 \tValidation Loss: 0.138792\n",
      "Epoch: 5 \tTraining Loss: 0.555170 \tValidation Loss: 0.138792\n",
      "Epoch: 6 \tTraining Loss: 0.555130 \tValidation Loss: 0.138792\n",
      "Epoch: 7 \tTraining Loss: 0.555171 \tValidation Loss: 0.138792\n",
      "Epoch: 8 \tTraining Loss: 0.555151 \tValidation Loss: 0.138792\n",
      "Epoch: 9 \tTraining Loss: 0.555138 \tValidation Loss: 0.138792\n",
      "Epoch: 10 \tTraining Loss: 0.555154 \tValidation Loss: 0.138792\n",
      "Epoch: 11 \tTraining Loss: 0.555153 \tValidation Loss: 0.138792\n",
      "Validation loss decreased (0.138792 --> 0.138792).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.555133 \tValidation Loss: 0.138792\n",
      "Epoch: 13 \tTraining Loss: 0.555149 \tValidation Loss: 0.138792\n",
      "Epoch: 14 \tTraining Loss: 0.555143 \tValidation Loss: 0.138792\n",
      "Epoch: 15 \tTraining Loss: 0.555169 \tValidation Loss: 0.138792\n",
      "Epoch: 16 \tTraining Loss: 0.555172 \tValidation Loss: 0.138792\n",
      "Epoch: 17 \tTraining Loss: 0.555166 \tValidation Loss: 0.138792\n",
      "Epoch: 18 \tTraining Loss: 0.555142 \tValidation Loss: 0.138792\n",
      "Epoch: 19 \tTraining Loss: 0.555139 \tValidation Loss: 0.138792\n",
      "Epoch: 20 \tTraining Loss: 0.555154 \tValidation Loss: 0.138792\n",
      "Epoch: 21 \tTraining Loss: 0.555111 \tValidation Loss: 0.138792\n",
      "Epoch: 22 \tTraining Loss: 0.555160 \tValidation Loss: 0.138792\n",
      "Epoch: 23 \tTraining Loss: 0.555151 \tValidation Loss: 0.138792\n",
      "Epoch: 24 \tTraining Loss: 0.555133 \tValidation Loss: 0.138792\n",
      "Epoch: 25 \tTraining Loss: 0.555150 \tValidation Loss: 0.138792\n",
      "Epoch: 26 \tTraining Loss: 0.555104 \tValidation Loss: 0.138792\n",
      "Epoch: 27 \tTraining Loss: 0.555173 \tValidation Loss: 0.138792\n",
      "Epoch: 28 \tTraining Loss: 0.555124 \tValidation Loss: 0.138792\n",
      "Epoch: 29 \tTraining Loss: 0.555199 \tValidation Loss: 0.138792\n",
      "Epoch: 30 \tTraining Loss: 0.555151 \tValidation Loss: 0.138792\n",
      "Epoch: 31 \tTraining Loss: 0.555184 \tValidation Loss: 0.138792\n",
      "Epoch: 32 \tTraining Loss: 0.555135 \tValidation Loss: 0.138792\n",
      "Epoch: 33 \tTraining Loss: 0.555113 \tValidation Loss: 0.138792\n",
      "Epoch: 34 \tTraining Loss: 0.555182 \tValidation Loss: 0.138792\n",
      "Epoch: 35 \tTraining Loss: 0.555131 \tValidation Loss: 0.138792\n",
      "Epoch: 36 \tTraining Loss: 0.555112 \tValidation Loss: 0.138792\n",
      "Epoch: 37 \tTraining Loss: 0.555131 \tValidation Loss: 0.138792\n",
      "Epoch: 38 \tTraining Loss: 0.555147 \tValidation Loss: 0.138792\n",
      "Epoch: 39 \tTraining Loss: 0.555167 \tValidation Loss: 0.138792\n",
      "Epoch: 40 \tTraining Loss: 0.555133 \tValidation Loss: 0.138792\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 50 %\n"
     ]
    }
   ],
   "source": [
    "#WV model binary classification accuracy for concatenated vectors using Feedforward neural network\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_loader, valid_loader, test_loader = prepareTrainandTest_ten(avg_my_model_x_train, avg_my_model_y_train, avg_my_model_x_test, avg_my_model_y_test,mymodel.wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.574479 \tValidation Loss: 0.142603\n",
      "Validation loss decreased (inf --> 0.142603).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.574554 \tValidation Loss: 0.142603\n",
      "Epoch: 3 \tTraining Loss: 0.574687 \tValidation Loss: 0.142603\n",
      "Epoch: 4 \tTraining Loss: 0.574609 \tValidation Loss: 0.142603\n",
      "Epoch: 5 \tTraining Loss: 0.574656 \tValidation Loss: 0.142603\n",
      "Epoch: 6 \tTraining Loss: 0.574843 \tValidation Loss: 0.142603\n",
      "Epoch: 7 \tTraining Loss: 0.574786 \tValidation Loss: 0.142603\n",
      "Validation loss decreased (0.142603 --> 0.142603).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.574532 \tValidation Loss: 0.142603\n",
      "Epoch: 9 \tTraining Loss: 0.574749 \tValidation Loss: 0.142603\n",
      "Epoch: 10 \tTraining Loss: 0.574592 \tValidation Loss: 0.142603\n",
      "Epoch: 11 \tTraining Loss: 0.574588 \tValidation Loss: 0.142603\n",
      "Epoch: 12 \tTraining Loss: 0.574543 \tValidation Loss: 0.142603\n",
      "Epoch: 13 \tTraining Loss: 0.574550 \tValidation Loss: 0.142603\n",
      "Epoch: 14 \tTraining Loss: 0.574672 \tValidation Loss: 0.142603\n",
      "Epoch: 15 \tTraining Loss: 0.574427 \tValidation Loss: 0.142603\n",
      "Epoch: 16 \tTraining Loss: 0.574400 \tValidation Loss: 0.142603\n",
      "Epoch: 17 \tTraining Loss: 0.574558 \tValidation Loss: 0.142603\n",
      "Epoch: 18 \tTraining Loss: 0.574543 \tValidation Loss: 0.142603\n",
      "Validation loss decreased (0.142603 --> 0.142603).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.574421 \tValidation Loss: 0.142603\n",
      "Epoch: 20 \tTraining Loss: 0.574596 \tValidation Loss: 0.142603\n",
      "Epoch: 21 \tTraining Loss: 0.574704 \tValidation Loss: 0.142603\n",
      "Epoch: 22 \tTraining Loss: 0.574551 \tValidation Loss: 0.142603\n",
      "Epoch: 23 \tTraining Loss: 0.574488 \tValidation Loss: 0.142603\n",
      "Epoch: 24 \tTraining Loss: 0.574719 \tValidation Loss: 0.142603\n",
      "Epoch: 25 \tTraining Loss: 0.574586 \tValidation Loss: 0.142603\n",
      "Epoch: 26 \tTraining Loss: 0.574568 \tValidation Loss: 0.142603\n",
      "Epoch: 27 \tTraining Loss: 0.574860 \tValidation Loss: 0.142603\n",
      "Epoch: 28 \tTraining Loss: 0.574415 \tValidation Loss: 0.142603\n",
      "Epoch: 29 \tTraining Loss: 0.574667 \tValidation Loss: 0.142603\n",
      "Epoch: 30 \tTraining Loss: 0.574579 \tValidation Loss: 0.142603\n",
      "Epoch: 31 \tTraining Loss: 0.574548 \tValidation Loss: 0.142603\n",
      "Epoch: 32 \tTraining Loss: 0.574678 \tValidation Loss: 0.142603\n",
      "Epoch: 33 \tTraining Loss: 0.574525 \tValidation Loss: 0.142603\n",
      "Epoch: 34 \tTraining Loss: 0.574637 \tValidation Loss: 0.142603\n",
      "Epoch: 35 \tTraining Loss: 0.574449 \tValidation Loss: 0.142603\n",
      "Epoch: 36 \tTraining Loss: 0.574491 \tValidation Loss: 0.142603\n",
      "Epoch: 37 \tTraining Loss: 0.574388 \tValidation Loss: 0.142603\n",
      "Validation loss decreased (0.142603 --> 0.142603).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.574644 \tValidation Loss: 0.142603\n",
      "Epoch: 39 \tTraining Loss: 0.574610 \tValidation Loss: 0.142603\n",
      "Epoch: 40 \tTraining Loss: 0.574703 \tValidation Loss: 0.142603\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 50 %\n"
     ]
    }
   ],
   "source": [
    "#Custom model binary classification accuracy for concatenated vectors using Feedforward neural network\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNN_Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=False)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = RNN_Model(input_size=300, output_size=2, hidden_dim=50, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "lr=0.001\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genpretrain_rnn(s,wv):\n",
    "    list = pretrain_rnn(s,wv)\n",
    "    list = np.array(list)\n",
    "    #list = list.astype(np.float64)\n",
    "    #list = list.mean(axis=0)\n",
    "    return list\n",
    "\n",
    "def pretrain_rnn(s,wv):\n",
    "    j = s.split()\n",
    "    if len(j)>20:\n",
    "        j = j[:20]\n",
    "        list = []\n",
    "        for word in j :\n",
    "            if word not in wv:\n",
    "                list.append([0 for i in range(300)])\n",
    "            else:\n",
    "                list.append(wv[word])\n",
    "    else:\n",
    "        diff = 20 - len(j)\n",
    "        list = []\n",
    "        for word in j :\n",
    "            if word not in wv:\n",
    "                list.append([0 for i in range(300)])\n",
    "            else:\n",
    "                list.append(wv[word])\n",
    "                \n",
    "        for i in range(diff):\n",
    "            list.append([0 for i in range(300)])\n",
    "    \n",
    "    return list\n",
    "\n",
    "def normalize_rnn(df):\n",
    "    rlist= []\n",
    "    for i in df:\n",
    "        list =[]\n",
    "        for j in i:\n",
    "            list.append(np.array(j))\n",
    "        rlist.append(np.array(list))\n",
    "    rlist = np.array(rlist)\n",
    "    return rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_rnn_binary = pd.DataFrame()\n",
    "mymodel_rnn_binary = pd.DataFrame()\n",
    "\n",
    "wv_rnn_binary['review_body']=df_binary['review_body'].apply(lambda row: genpretrain_rnn(row, wv))\n",
    "mymodel_rnn_binary['review_body']=df_binary['review_body'].apply(lambda row: genpretrain_rnn(row, mymodel.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wv_trained=normalize_rnn(wv_rnn_binary['review_body'])\n",
    "n_mymodel_trained=normalize_rnn(mymodel_rnn_binary['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_my_model_x_train, avg_my_model_x_test, avg_my_model_y_train, avg_my_model_y_test = train_test_split(n_mymodel_trained, df_ternary[\"rating\"],test_size=0.2, random_state=0)\n",
    "avg_wv_x_train, avg_wv_x_test, avg_wv_y_train, avg_wv_y_test = train_test_split(n_wv_trained, df_ternary[\"rating\"],test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_loader, valid_loader, test_loader = prepareTrainandTest_ten(avg_my_model_x_train, avg_my_model_y_train, avg_my_model_x_test, avg_my_model_y_test,mymodel.wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test images: 64%\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_loader, valid_loader, test_loader = prepareTrainandTest_ten(avg_wv_x_train,avg_wv_y_train,avg_wv_x_test,avg_wv_y_test,wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test images: 62%\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlphw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
